{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常用图形操作\n",
    "#matplotlib\n",
    "\n",
    "1. 显示图片\n",
    "import matplotlib.pyplot as plt # plt 用于显示图片\n",
    "import matplotlib.image as mpimg # mpimg 用于读取图片\n",
    "import numpy as np\n",
    "\n",
    "lena = mpimg.imread('lena.png') # 读取和代码处于同一目录下的 lena.png\n",
    "#此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "lena.shape #(512, 512, 3)\n",
    "\n",
    "plt.imshow(lena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()\n",
    "\n",
    "2. 显示某个通道\n",
    "#显示图片的第一个通道\n",
    "lena_1 = lena[:,:,0]\n",
    "plt.imshow('lena_1')\n",
    "plt.show()\n",
    "#此时会发现显示的是热量图，不是我们预想的灰度图，可以添加 cmap 参数，有如下几种添加方法：\n",
    "plt.imshow('lena_1', cmap='Greys_r')\n",
    "plt.show()\n",
    "\n",
    "img = plt.imshow('lena_1')\n",
    "img.set_cmap('gray') # 'hot' 是热量图\n",
    "plt.show()\n",
    "\n",
    "3. 将 RGB 转为灰度图\n",
    "matplotlib 中没有合适的函数可以将 RGB 图转换为灰度图，可以根据公式自定义一个：\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "gray = rgb2gray(lena)    \n",
    "#也可以用 plt.imshow(gray, cmap = plt.get_cmap('gray'))\n",
    "plt.imshow(gray, cmap='Greys_r')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "4. 对图像进行放缩\n",
    "这里要用到 scipy\n",
    "from scipy import misc\n",
    "lena_new_sz = misc.imresize(lena, 0.5) # 第二个参数如果是整数，则为百分比，如果是tuple，则为输出图像的尺寸\n",
    "plt.imshow(lena_new_sz)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二、PIL\n",
    "\n",
    "1. 显示图片\n",
    "from PIL import Image\n",
    "im = Image.open('lena.png')\n",
    "im.show()\n",
    "\n",
    "2. 将 PIL Image 图片转换为 numpy 数组\n",
    "im_array = np.array(im)\n",
    "#也可以用 np.asarray(im) 区别是 np.array() 是深拷贝，np.asarray() 是浅拷贝\n",
    "\n",
    "3. 保存 PIL 图片\n",
    "直接调用 Image 类的 save 方法\n",
    "from PIL import Image\n",
    "I = Image.open('lena.png')\n",
    "I.save('new_lena.png')\n",
    "\n",
    "4. 将 numpy 数组转换为 PIL 图片\n",
    "这里采用 matplotlib.image 读入图片数组，注意这里读入的数组是 float32 型的，范围是 0-1，而 PIL.Image 数据是 uinit8 型的，范围是0-255，所以要进行转换：\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "lena = mpimg.imread('lena.png') # 这里读入的数据是 float32 型的，范围是0-1\n",
    "im = Image.fromarray(np.uinit8(lena*255))\n",
    "im.show()\n",
    "\n",
    " 5. RGB 转换为灰度图\n",
    "from PIL import Image\n",
    "I = Image.open('lena.png')\n",
    "I.show()\n",
    "L = I.convert('L')\n",
    "L.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV2\n",
    "1.读取\n",
    " 读入时可制定\n",
    "cvim=cv2.imread(chosedfile[0],cv2.CV_LOAD_IMAGE_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.datasets import mnist\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import cv2\n",
    "import math\n",
    "from scipy import ndimage\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # unnormalize Inception v3 \t22.55 \t6.44\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    \n",
    "def getBestShift(img):\n",
    "    cy,cx = ndimage.measurements.center_of_mass(img)\n",
    "    rows,cols = img.shape\n",
    "    shiftx = np.round(cols/2.0-cx).astype(int)\n",
    "    shifty = np.round(rows/2.0-cy).astype(int)\n",
    "    return shiftx,shifty\n",
    "    \n",
    "def shift(img,sx,sy):\n",
    "    rows,cols = img.shape\n",
    "    M = np.float32([[1,0,sx],[0,1,sy]])\n",
    "    shifted = cv2.warpAffine(img,M,(cols,rows))\n",
    "    return shifted    \n",
    "  \n",
    "def centrelize(gray): # move a char, black as background, and white as foreground,  to center of a 28*28 image\n",
    "    while np.sum(gray[0]) == 0:\n",
    "        gray = gray[1:]\n",
    "        \n",
    "    while np.sum(gray[:,0]) == 0:\n",
    "        gray = np.delete(gray,0,1)\n",
    "\n",
    "    while np.sum(gray[-1]) == 0:\n",
    "        gray = gray[:-1]\n",
    "        \n",
    "    while np.sum(gray[:,-1]) == 0:\n",
    "        gray = np.delete(gray,-1,1)\n",
    "\n",
    "    rows,cols = gray.shape\n",
    "        \n",
    "    if rows>cols:\n",
    "        factor = 20.0/rows\n",
    "        rows = 20\n",
    "        cols = int(round(cols*factor))\n",
    "        gray = cv2.resize(gray,(cols,rows))\n",
    "    else:\n",
    "        factor = 20.0/cols\n",
    "        cols = 20\n",
    "        rows = int(round(rows*factor))\n",
    "        gray = cv2.resize(gray,(cols,rows))\n",
    "\n",
    "    colsPadding = (int(math.ceil((28-cols)/2.0)),int(math.floor((28-cols)/2.0)))\n",
    "    rowsPadding = (int(math.ceil((28-rows)/2.0)),int(math.floor((28-rows)/2.0)))\n",
    "    gray = np.lib.pad(gray,(rowsPadding,colsPadding),'constant')\n",
    "    shiftx,shifty = getBestShift(gray)\n",
    "    shifted = shift(gray,shiftx,shifty)\n",
    "    \n",
    "    return shifted    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PICSET(data.Dataset):\n",
    "    picroot='pic'\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img=self.dataset[index]\n",
    "        img=Image.fromarray(img.numpy(), mode='L')\n",
    "        if self.transform is not None:\n",
    "            img=self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __init__(self,root,transform=None):\n",
    "        self.picroot=root\n",
    "        self.transform=transform\n",
    "        self.kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(3, 3))\n",
    "        \n",
    "        if not os.path.exists(self.picroot):\n",
    "            raise RuntimeError('{} doesnot exists'.format(self.picroot))\n",
    "        for root,dnames,filenames in os.walk(self.picroot):\n",
    "            imgs=np.ndarray(shape=(len(filenames),28,28),dtype=np.float)\n",
    "            i=0\n",
    "            for filename in filenames:\n",
    "                picfilename=os.path.join(self.picroot,filename)\n",
    "                im=cv2.imread(picfilename,cv2.IMREAD_GRAYSCALE)\n",
    "                im=cv2.resize(255-im,(28,28))\n",
    "                (thresh, im) = cv2.threshold(im, 32, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)   \n",
    "                #im=cv2.erode(im,self.kernel)\n",
    "                #im=cv2.dilate(im,self.kernel)\n",
    "                #im=cv2.GaussianBlur(im,(5,5),0.1)\n",
    "                im=centrelize(im)\n",
    "                (thresh, im) = cv2.threshold(im, 32, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)   \n",
    "                imgs[i]=im/255\n",
    "                i=i+1\n",
    "            self.dataset=torch.FloatTensor(imgs)\n",
    "            self.len=len(filenames)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self):    \n",
    "        super(Net1, self).__init__()        \n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool  = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*4*4, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "        self.conv2drop=nn.Dropout2d()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2drop(self.conv2(x))))\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= sos.path\n",
    "        return num_features\n",
    "\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):    \n",
    "        super(Net2, self).__init__()        \n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)        \n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= sos.path\n",
    "        return num_features\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model=Net1()\n",
    "#model.load_state_dict(torch.load('/home/wang/git/cifar10/mnist.weight'))\n",
    "model=Model()\n",
    "model.load_state_dict(torch.load('/home/wang/git/cifar10/mnist.weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABMCAYAAAB9PUwnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACKlJREFUeJzt3U+oXOUZx/Hvr/HPorpQ7uUSYm6TQjZZRQlWsIhQ2tpu\nYjeSFiQLIV3EolAXt250aQt1W0gxkIVtCKiYhVSsCKUbm0TSJjchmtoEE2JS6ULpohL7dDHnxuN0\nzsyZf+e875nfB4Y7c2bmnmeeec4z77xz5owiAjMzy9/X2g7AzMxmww3dzKwj3NDNzDrCDd3MrCPc\n0M3MOsIN3cysI6Zq6JIekXRe0gVJa7MKyszMxqdJ90OXtAl4H/gucBk4Dvw4Is7OLjwzM6trmhH6\n/cCFiPgwIj4HjgB7ZhOWmZmN65Yp7rsF+Kh0+TLwrWF3WFpaim3btk2xSjOzxXPy5MlPImJ51O2m\naei1SNoP7AdYXV3lxIkT816lmVmnSLpU53bTTLlcAbaWLt9TLPuKiDgYEbsjYvfy8sgXGDMzm9A0\nI/TjwA5J2+k18r3AT+reWdIUq56v8gfFjnN6G3HmECM4zllwnO2YuKFHxA1JTwJvApuAQxGxPrPI\nJtS/104XniQzS9uwvQWb7EFTzaFHxBvAGzOKxczMpjD3D0Wb5GO7m9m8jdtnmpzW6URDz6GRpzYV\nNE7O2o61rCrulGLMRWo1WZZrfY4SEXONN/uGnsrcVU5SHmHUjWPQdU3HVjePrsP6JhmcpVKfdc1z\nJwEfnMvMrCOyH6FXyeGVOkcpj4RT3T0ylxHkqDynHPuGNuqzn6TWpoGzbuiDktb2k5mjQTkbNb1R\ndb8UpLBRV0k5thRV5arN6bc6/7t8m6pY5xGnp1zMzDoi6xG6TWZWI4w2bcTXZmyTjB5tuLoj1tTr\ns6zJWLNs6N51rVkpNM8qKcaWYkx15Lr9tDlnnZrsGrqfOIMv5x9TrodUG3tq8djseA7dzKwjshuh\nD5PyN99yluqILtW4rFmp10GT8WXT0BfhG2QpSn1jGSSV5zqX3KWSr3Hlkt8mecrFzKwjshmhT8tf\n6BhPSsfISfXDRUgzpnGl9FzXlUvMTddH1g193G84Wj25bCz9Uo5tQw4xlqX4reAc6rNOH5pHrFk0\n9EkPpdl/P4/SR0txY8n9WC5ludZgCnGnWJsbUjncr+fQzcw6IosRer+2X427KOXRT9e0Pdodte4U\npy1Trs+Ujos/coQuaaukdySdlbQu6ali+d2S3pL0QfH3rrlHO6a2n+jURcTNU5VUcygpidg24ug/\n5azqMTTd6EfVZxdyPWt1plxuAD+PiJ3AA8ABSTuBNeDtiNgBvF1cNjOzloxs6BFxNSLeK85/BpwD\ntgB7gMPFzQ4Dj84rSGteKqOfFN/+1zFqtD7qnZEN1/ZPIdZ5d7uhyW1prDl0SduAe4F3gZWIuFpc\n9TGwMtPIhqi7N4MPBVAtx2mWutqeo+6X+kHEqrQZd4r1mdJceZXaDV3SHcArwNMR8Wnf7oEhaeCj\nlbQf2A+wuro6XbR9hm24OW5AKUipEdaR8peOytqOswu/7pVLvLN4jid9rLV2W5R0K71m/nJEvFos\nviZpc3H9ZuD6oPtGxMGI2B0Ru5eXlycK0szMRquzl4uAl4BzEfFi6apjwL7i/D7g9dmHdzOGyuv6\n57M8Nzla1WgtlxGQdVfV9uvarKfOlMuDwOPAaUmnimXPAi8ARyU9AVwCHptPiLPhghiura8qTyq1\nefJRUhxkjMphSjGPiiWnWpinkQ09Iv4MVGXrO7MNx8zMJpXNN0Un/aFVv3LPTht7Hgzb0yKlEeQw\nKceZcmzjSHGvmDZk09DLFukJsvE1WR/D5nu70iytJ4e+k2VDt+mMW5htN6ZJ352lLMdjyjcR8yTr\n6EpNzIKPtmhm1hEeoTckh7drVVKKPaVYJpXykRZz1MXHNCk3dLMpDGsmbjTWNE+5mJl1hBu6mVlH\nqMlPiCX9E/g38EljK83DEs7JIM7LYM7LYF3OyzciYuTBsBpt6ACSTkTE7kZXmjjnZDDnZTDnZTDn\nxVMuZmad4YZuZtYRbTT0gy2sM3XOyWDOy2DOy2ALn5fG59DNzGw+POViZtYRbuhmZh3RWEOX9Iik\n85IuSFprar0pknRR0mlJpySdKJbdLektSR8Uf+9qO855k3RI0nVJZ0rLKvMg6RdF/ZyX9P12op6v\nipw8L+lKUS+nJP2wdF3ncwIgaaukdySdlbQu6ali+ULXy/+p+k3OWZ6ATcDfgW8CtwF/BXY2se4U\nT8BFYKlv2a+AteL8GvDLtuNsIA8PAfcBZ0blAdhZ1M3twPainja1/RgaysnzwDMDbrsQOSke62bg\nvuL8ncD7xeNf6HrpPzU1Qr8fuBARH0bE58ARYE9D687FHuBwcf4w8GiLsTQiIv4E/KtvcVUe9gBH\nIuI/EfEP4AK9uuqUipxUWYicAETE1Yh4rzj/GXAO2MKC10u/phr6FuCj0uXLxbJFFcAfJZ2UtL9Y\nthIRV4vzHwMr7YTWuqo8LHoN/UzS34opmY1phYXMiaRtwL3Au7hevsIfirbj2xGxC/gBcEDSQ+Ur\no/eeceH3J3UebvoNvenKXcBV4NfthtMeSXcArwBPR8Sn5etcL8019CvA1tLle4plCykirhR/rwOv\n0XsreE3SZoDi7/X2ImxVVR4WtoYi4lpEfBER/wV+y5dTBwuVE0m30mvmL0fEq8Vi10tJUw39OLBD\n0nZJtwF7gWMNrTspkr4u6c6N88D3gDP08rGvuNk+4PV2ImxdVR6OAXsl3S5pO7AD+EsL8TVuo2EV\nfkSvXmCBcqLer4W8BJyLiBdLV7leShr5xaKIuCHpSeBNenu8HIqI9SbWnaAV4LXi12xuAX4XEX+Q\ndBw4KukJ4BLwWIsxNkLS74GHgSVJl4HngBcYkIeIWJd0FDgL3AAORMQXrQQ+RxU5eVjSLnrTCReB\nn8Li5KTwIPA4cFrSqWLZsyx4vfTzV//NzDrCH4qamXWEG7qZWUe4oZuZdYQbuplZR7ihm5l1hBu6\nmVlHuKGbmXXE/wAxv3vwEB/3HAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7dd392b790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#图像来自pic目录\n",
    "batch=8\n",
    "inputarray=np.ndarray(shape=(batch,1,28,28),dtype=float)\n",
    "files=os.listdir('pic')\n",
    "chosedfile={}.fromkeys(range(batch))\n",
    "for k in chosedfile:\n",
    "    chosedfile[k]=os.path.join('pic',random.choice(files))\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(3, 3))\n",
    "for k in chosedfile:\n",
    "    im=cv2.imread(chosedfile[k],cv2.CV_LOAD_IMAGE_GRAYSCALE)\n",
    "    gray=cv2.resize(255-im,(28,28))\n",
    "    (thresh, gray) = cv2.threshold(gray, 32, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)   \n",
    "    gray=centrelize(gray)\n",
    "    (thresh, gray) = cv2.threshold(gray, 32, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)   \n",
    "    gray=cv2.dilate(gray,kernel)\n",
    "    gray=gray/255\n",
    "    gray=(gray-0.5)*2\n",
    "    inputarray[k][0]=gray\n",
    "\n",
    "inputs=torch.FloatTensor(inputarray)\n",
    "imshow(torchvision.utils.make_grid(inputs))    \n",
    "\n",
    "#---------自行构造从目录读图的dataset以便使用dataloader\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                             ])\n",
    "#picset = PICSET(root='pic', transform=transform)\n",
    "#picloader = torch.utils.data.DataLoader(picset, batch_size=batch, shuffle=False, num_workers=1)\n",
    "#piciter = iter(picloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    6\n",
      "    1\n",
      "    2\n",
      "    8\n",
      "    3\n",
      "    5\n",
      "    2\n",
      "    2\n",
      "[torch.IntTensor of size 8x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mages = piciter.next()\n",
    "#=images.numpy()\n",
    "#rint a.shape\n",
    "#rint a[0][0]\n",
    "#mshow(torchvision.utils.make_grid(torch.FloatTensor(images.numpy()/2+0.5)))\n",
    "\n",
    "#images = torch.FloatTensor((1-(images.numpy()/2+0.5)-0.5)*2)\n",
    "       \n",
    "#imshow(torchvision.utils.make_grid(torch.FloatTensor((1-(images.numpy()/2+0.5)-0.5)*2)))     #from [-1,1] to [0,1] and to[-1,1]   \n",
    "\n",
    "outputs = model(Variable(inputs))\n",
    "_,predicted = torch.max(outputs.data, 1)\n",
    "#print chosedfile\n",
    "print predicted.int()\n",
    "#print classes[predicted.int()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded\n"
     ]
    }
   ],
   "source": [
    "#图像来自mnist test set\n",
    "#testset = mnist.MNIST(root='MNIST', train=False, download=True)\n",
    "#b=random.randint(0,mnist.MNIST.__len__(testset)-batch)\n",
    "#for i in range(0,batch):\n",
    "    #timg,tlabel=mnist.MNIST.__getitem__(testset,b+i)  #timg is PIL.Image\n",
    "    #inputarray[i][0]=np.asarray(list(timg.getdata())).reshape((28,28))\n",
    "    #inputarray[i][0]=inputarray[i][0]/255\n",
    "    #inputarray[i][0]=(inputarray[i][0]-0.5)*2\n",
    "#inputs=torch.FloatTensor(inputarray)\n",
    "#imshow(torchvision.utils.make_grid(inputs))        \n",
    "#-----------------------------------\n",
    "testset = mnist.MNIST(root='MNIST', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch, shuffle=False, num_workers=2)\n",
    "mnistiter = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABMCAYAAAB9PUwnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYAgRjEE3GAyGEOH/AfnXZR8iLC37AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7dd0195f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = mnistiter.next()\n",
    "imshow(torchvision.utils.make_grid(images)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    5\n",
      "    9\n",
      "    0\n",
      "    6\n",
      "    9\n",
      "    0\n",
      "    1\n",
      "    5\n",
      "[torch.IntTensor of size 8x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = model(Variable(images))\n",
    "_,predicted = torch.max(outputs.data, 1)\n",
    "print predicted.int()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
