{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常用图形操作\n",
    "#matplotlib\n",
    "\n",
    "1. 显示图片\n",
    "import matplotlib.pyplot as plt # plt 用于显示图片\n",
    "import matplotlib.image as mpimg # mpimg 用于读取图片\n",
    "import numpy as np\n",
    "\n",
    "lena = mpimg.imread('lena.png') # 读取和代码处于同一目录下的 lena.png\n",
    "#此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "lena.shape #(512, 512, 3)\n",
    "\n",
    "plt.imshow(lena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()\n",
    "\n",
    "2. 显示某个通道\n",
    "#显示图片的第一个通道\n",
    "lena_1 = lena[:,:,0]\n",
    "plt.imshow('lena_1')\n",
    "plt.show()\n",
    "#此时会发现显示的是热量图，不是我们预想的灰度图，可以添加 cmap 参数，有如下几种添加方法：\n",
    "plt.imshow('lena_1', cmap='Greys_r')\n",
    "plt.show()\n",
    "\n",
    "img = plt.imshow('lena_1')\n",
    "img.set_cmap('gray') # 'hot' 是热量图\n",
    "plt.show()\n",
    "\n",
    "3. 将 RGB 转为灰度图\n",
    "matplotlib 中没有合适的函数可以将 RGB 图转换为灰度图，可以根据公式自定义一个：\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "gray = rgb2gray(lena)    \n",
    "#也可以用 plt.imshow(gray, cmap = plt.get_cmap('gray'))\n",
    "plt.imshow(gray, cmap='Greys_r')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "4. 对图像进行放缩\n",
    "这里要用到 scipy\n",
    "from scipy import misc\n",
    "lena_new_sz = misc.imresize(lena, 0.5) # 第二个参数如果是整数，则为百分比，如果是tuple，则为输出图像的尺寸\n",
    "plt.imshow(lena_new_sz)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二、PIL\n",
    "\n",
    "1. 显示图片\n",
    "from PIL import Image\n",
    "im = Image.open('lena.png')\n",
    "im.show()\n",
    "\n",
    "2. 将 PIL Image 图片转换为 numpy 数组\n",
    "im_array = np.array(im)\n",
    "#也可以用 np.asarray(im) 区别是 np.array() 是深拷贝，np.asarray() 是浅拷贝\n",
    "\n",
    "3. 保存 PIL 图片\n",
    "直接调用 Image 类的 save 方法\n",
    "from PIL import Image\n",
    "I = Image.open('lena.png')\n",
    "I.save('new_lena.png')\n",
    "\n",
    "4. 将 numpy 数组转换为 PIL 图片\n",
    "这里采用 matplotlib.image 读入图片数组，注意这里读入的数组是 float32 型的，范围是 0-1，而 PIL.Image 数据是 uinit8 型的，范围是0-255，所以要进行转换：\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "lena = mpimg.imread('lena.png') # 这里读入的数据是 float32 型的，范围是0-1\n",
    "im = Image.fromarray(np.uinit8(lena*255))\n",
    "im.show()\n",
    "\n",
    " 5. RGB 转换为灰度图\n",
    "from PIL import Image\n",
    "I = Image.open('lena.png')\n",
    "I.show()\n",
    "L = I.convert('L')\n",
    "L.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV2\n",
    "1.读取\n",
    " 读入时可制定\n",
    "cvim=cv2.imread(chosedfile[0],cv2.CV_LOAD_IMAGE_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.datasets import mnist\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import cv2\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # unnormalize Inception v3 \t22.55 \t6.44\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PICSET(data.Dataset):\n",
    "    picroot='pic'\n",
    "    \n",
    "    def __init__(self,root,transform=None):\n",
    "        self.picroot=root\n",
    "        self.transform=transform\n",
    "        self.kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(3, 3))\n",
    "        \n",
    "        if not os.path.exists(self.picroot):\n",
    "            raise RuntimeError('{} doesnot exists'.format(self.picroot))\n",
    "        for root,dnames,filenames in os.walk(self.picroot):\n",
    "            imgs=np.ndarray(shape=(len(filenames),28,28),dtype=np.uint8)\n",
    "            i=0\n",
    "            for filename in filenames:\n",
    "                picfilename=os.path.join(self.picroot,filename)\n",
    "                im=cv2.imread(picfilename,cv2.CV_LOAD_IMAGE_GRAYSCALE)\n",
    "                im=cv2.erode(cv2.resize(im,(28,28)),self.kernel)\n",
    "                im=cv2.erode(im,self.kernel)\n",
    "                im=cv2.dilate(im,self.kernel)\n",
    "                im=cv2.GaussianBlur(im,(5,5),0.1)\n",
    "                imgs[i]=im\n",
    "                #imgs[i]=cv2.resize(im,(28,28))\n",
    "                i=i+1\n",
    "            self.dataset=torch.ByteTensor(imgs)\n",
    "            self.len=len(filenames)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img=self.dataset[index]\n",
    "        img=Image.fromarray(img.numpy(), mode='L')\n",
    "        if self.transform is not None:\n",
    "            img=self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):    \n",
    "\n",
    "        super(Net, self).__init__()        \n",
    "        #self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        #self.pool  = nn.MaxPool2d(2,2)\n",
    "        #self.conv2 = nn.Conv2d(6, 16, 5)outputs = model(Variable(inputs))\n",
    "        #self.fc1   = nn.Linear(16*4*4, 120)\n",
    "        #self.fc2   = nn.Linear(120, 84)\n",
    "        #self.fc3   = nn.Linear(84, 10)\n",
    "        #self.conv2drop=nn.Dropout2d()outputs = model(Variable(inputs))\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = F.relu(self.pool(self.conv1(x)))\n",
    "        #x = F.relu(self.pool(self.conv2drop(self.conv2(x))))\n",
    "        #x = x.view(-1, 16*4*4)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        #x = self.fc3(x)\n",
    "        #return F.log_softmax(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)        \n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= sos.path\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=Net()\n",
    "model.load_state_dict(torch.load('/home/wang/git/cifar10/mnist.weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#图像来自pic目录\n",
    "batch=3\n",
    "#inputarray=np.ndarray(shape=(batch,1,28,28),dtype=float)\n",
    "#files=os.listdir('pic')\n",
    "#chosedfile={}.fromkeys(range(batch))\n",
    "#for k in chosedfile:\n",
    "#    chosedfile[k]=os.path.join('pic',random.choice(files))\n",
    "\n",
    "#kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(3, 3))\n",
    "#for k in chosedfile:\n",
    "#    im=cv2.imread(chosedfile[k],cv2.CV_LOAD_IMAGE_GRAYSCALE)\n",
    "#    gray=cv2.resize(im,(28,28))\n",
    "#    gray=gray/255\n",
    "#    gray=(np.int_(np.logical_not(gray))-0.5)*2\n",
    "#    #gray=cv2.GaussianBlur(gray,(5,5),0.1)\n",
    "#    #gray=(gray-0.5)*2.view(-1,28,28)\n",
    "#    inputarray[k][0]=gray\n",
    "    \n",
    "#inputs=torch.FloatTensor(inputarray)\n",
    "#imshow(torchvision.utils.make_grid(inputs))    \n",
    "\n",
    "#---------自行构造从目录读图的dataset以便使用loader\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                             ])\n",
    "picset = PICSET(root='pic', transform=transform)\n",
    "picloader = torch.utils.data.DataLoader(picset, batch_size=batch, shuffle=False, num_workers=1)\n",
    "dataiter = iter(picloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACSCAYAAABVCTF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c584c1390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(torch.FloatTensor((1-(images.numpy()/2+0.5)-0.5)*2)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3\n",
      " 6\n",
      " 4\n",
      "[torch.IntTensor of size 3x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = model(Variable(images))\n",
    "_,predicted = torch.max(outputs.data, 1)\n",
    "#print chosedfile\n",
    "print predicted.int()\n",
    "#print classes[predicted.int()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded\n"
     ]
    }
   ],
   "source": [
    "#图像来自mnist test set\n",
    "#testset = mnist.MNIST(root='MNIST', train=False, download=True)\n",
    "#b=random.randint(0,mnist.MNIST.__len__(testset)-batch)\n",
    "#for i in range(0,batch):\n",
    "    #timg,tlabel=mnist.MNIST.__getitem__(testset,b+i)  #timg is PIL.Image\n",
    "    #inputarray[i][0]=np.asarray(list(timg.getdata())).reshape((28,28))\n",
    "    #inputarray[i][0]=inputarray[i][0]/255\n",
    "    #inputarray[i][0]=(inputarray[i][0]-0.5)*2\n",
    "#inputs=torch.FloatTensor(inputarray)\n",
    "#imshow(torchvision.utils.make_grid(inputs))        \n",
    "#-----------------------------------\n",
    "testset = mnist.MNIST(root='MNIST', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch, shuffle=False, num_workers=2)\n",
    "dataiter = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACSCAYAAABVCTF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c587af310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 7\n",
      " 2\n",
      " 1\n",
      "[torch.IntTensor of size 3x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = model(Variable(images))\n",
    "_,predicted = torch.max(outputs.data, 1)\n",
    "print predicted.int()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
