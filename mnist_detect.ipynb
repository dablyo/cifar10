{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常用图形操作\n",
    "#matplotlib\n",
    "\n",
    "1. 显示图片\n",
    "import matplotlib.pyplot as plt # plt 用于显示图片\n",
    "import matplotlib.image as mpimg # mpimg 用于读取图片\n",
    "import numpy as np\n",
    "\n",
    "lena = mpimg.imread('lena.png') # 读取和代码处于同一目录下的 lena.png\n",
    "#此时 lena 就已经是一个 np.array 了，可以对它进行任意处理\n",
    "lena.shape #(512, 512, 3)\n",
    "\n",
    "plt.imshow(lena) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()\n",
    "\n",
    "2. 显示某个通道\n",
    "#显示图片的第一个通道\n",
    "lena_1 = lena[:,:,0]\n",
    "plt.imshow('lena_1')\n",
    "plt.show()\n",
    "#此时会发现显示的是热量图，不是我们预想的灰度图，可以添加 cmap 参数，有如下几种添加方法：\n",
    "plt.imshow('lena_1', cmap='Greys_r')\n",
    "plt.show()\n",
    "\n",
    "img = plt.imshow('lena_1')\n",
    "img.set_cmap('gray') # 'hot' 是热量图\n",
    "plt.show()\n",
    "\n",
    "3. 将 RGB 转为灰度图\n",
    "matplotlib 中没有合适的函数可以将 RGB 图转换为灰度图，可以根据公式自定义一个：\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "gray = rgb2gray(lena)    \n",
    "#也可以用 plt.imshow(gray, cmap = plt.get_cmap('gray'))\n",
    "plt.imshow(gray, cmap='Greys_r')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "4. 对图像进行放缩\n",
    "这里要用到 scipy\n",
    "from scipy import misc\n",
    "lena_new_sz = misc.imresize(lena, 0.5) # 第二个参数如果是整数，则为百分比，如果是tuple，则为输出图像的尺寸\n",
    "plt.imshow(lena_new_sz)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二、PIL\n",
    "\n",
    "1. 显示图片\n",
    "from PIL import Image\n",
    "im = Image.open('lena.png')\n",
    "im.show()\n",
    "\n",
    "2. 将 PIL Image 图片转换为 numpy 数组\n",
    "im_array = np.array(im)\n",
    "#也可以用 np.asarray(im) 区别是 np.array() 是深拷贝，np.asarray() 是浅拷贝\n",
    "\n",
    "3. 保存 PIL 图片\n",
    "直接调用 Image 类的 save 方法\n",
    "from PIL import Image\n",
    "I = Image.open('lena.png')\n",
    "I.save('new_lena.png')\n",
    "\n",
    "4. 将 numpy 数组转换为 PIL 图片\n",
    "这里采用 matplotlib.image 读入图片数组，注意这里读入的数组是 float32 型的，范围是 0-1，而 PIL.Image 数据是 uinit8 型的，范围是0-255，所以要进行转换：\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "lena = mpimg.imread('lena.png') # 这里读入的数据是 float32 型的，范围是0-1\n",
    "im = Image.fromarray(np.uinit8(lena*255))\n",
    "im.show()\n",
    "\n",
    " 5. RGB 转换为灰度图\n",
    "from PIL import Image\n",
    "I = Image.open('lena.png')\n",
    "I.show()\n",
    "L = I.convert('L')\n",
    "L.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV2\n",
    "1.读取\n",
    " 读入时可制定\n",
    "cvim=cv2.imread(chosedfile[0],cv2.CV_LOAD_IMAGE_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.datasets import mnist\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import cv2\n",
    "import math\n",
    "from scipy import ndimage\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # unnormalize Inception v3 \t22.55 \t6.44\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    \n",
    "def getBestShift(img):\n",
    "    cy,cx = ndimage.measurements.center_of_mass(img)\n",
    "    rows,cols = img.shape\n",
    "    shiftx = np.round(cols/2.0-cx).astype(int)\n",
    "    shifty = np.round(rows/2.0-cy).astype(int)\n",
    "    return shiftx,shifty\n",
    "    \n",
    "def shift(img,sx,sy):\n",
    "    rows,cols = img.shape\n",
    "    M = np.float32([[1,0,sx],[0,1,sy]])\n",
    "    shifted = cv2.warpAffine(img,M,(cols,rows))\n",
    "    return shifted    \n",
    "  \n",
    "def centrelize(gray): # move a char, black as background, and white as foreground,  to center of a 28*28 image\n",
    "    while np.sum(gray[0]) == 0:\n",
    "        gray = gray[1:]\n",
    "        \n",
    "    while np.sum(gray[:,0]) == 0:\n",
    "        gray = np.delete(gray,0,1)\n",
    "\n",
    "    while np.sum(gray[-1]) == 0:\n",
    "        gray = gray[:-1]\n",
    "        \n",
    "    while np.sum(gray[:,-1]) == 0:\n",
    "        gray = np.delete(gray,-1,1)\n",
    "\n",
    "    rows,cols = gray.shape\n",
    "        \n",
    "    if rows>cols:\n",
    "        factor = 20.0/rows\n",
    "        rows = 20\n",
    "        cols = int(round(cols*factor))\n",
    "        gray = cv2.resize(gray,(cols,rows))\n",
    "    else:\n",
    "        factor = 20.0/cols\n",
    "        cols = 20\n",
    "        rows = int(round(rows*factor))\n",
    "        gray = cv2.resize(gray,(cols,rows))\n",
    "\n",
    "    colsPadding = (int(math.ceil((28-cols)/2.0)),int(math.floor((28-cols)/2.0)))\n",
    "    rowsPadding = (int(math.ceil((28-rows)/2.0)),int(math.floor((28-rows)/2.0)))\n",
    "    gray = np.lib.pad(gray,(rowsPadding,colsPadding),'constant')\n",
    "    shiftx,shifty = getBestShift(gray)\n",
    "    shifted = shift(gray,shiftx,shifty)\n",
    "    \n",
    "    return shifted    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PICSET(data.Dataset):\n",
    "    picroot='pic'\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img=self.dataset[index]\n",
    "        img=Image.fromarray(img.numpy(), mode='L')\n",
    "        if self.transform is not None:\n",
    "            img=self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __init__(self,root,transform=None):\n",
    "        self.picroot=root\n",
    "        self.transform=transform\n",
    "        self.kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(3, 3))\n",
    "        \n",
    "        if not os.path.exists(self.picroot):\n",
    "            raise RuntimeError('{} doesnot exists'.format(self.picroot))\n",
    "        for root,dnames,filenames in os.walk(self.picroot):\n",
    "            imgs=np.ndarray(shape=(len(filenames),28,28),dtype=np.float)\n",
    "            i=0\n",
    "            for filename in filenames:\n",
    "                picfilename=os.path.join(self.picroot,filename)\n",
    "                im=cv2.imread(picfilename,cv2.IMREAD_GRAYSCALE)\n",
    "                im=cv2.resize(255-im,(28,28))\n",
    "                (thresh, im) = cv2.threshold(im, 32, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)   \n",
    "                #im=cv2.erode(im,self.kernel)\n",
    "                #im=cv2.dilate(im,self.kernel)\n",
    "                #im=cv2.GaussianBlur(im,(5,5),0.1)\n",
    "                im=centrelize(im)\n",
    "                (thresh, im) = cv2.threshold(im, 32, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)   \n",
    "                imgs[i]=im/255\n",
    "                i=i+1\n",
    "            self.dataset=torch.FloatTensor(imgs)\n",
    "            self.len=len(filenames)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self):    \n",
    "        super(Net1, self).__init__()        \n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool  = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*4*4, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "        self.conv2drop=nn.Dropout2d()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2drop(self.conv2(x))))\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= sos.path\n",
    "        return num_features\n",
    "\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):    \n",
    "        super(Net2, self).__init__()        \n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)        \n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= sos.path\n",
    "        return num_features\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model=Net1()\n",
    "#model.load_state_dict(torch.load('/home/wang/git/cifar10/mnist.weight'))\n",
    "model=Model()\n",
    "model.load_state_dict(torch.load('/home/wang/git/cifar10/officalmnist.weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB2CAYAAADY3GjsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCs3EAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f57fc585450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#图像来自pic目录\n",
    "batch=4\n",
    "inputarray=np.ndarray(shape=(batch,1,28,28),dtype=float)\n",
    "files=os.listdir('pic')\n",
    "chosedfile={}.fromkeys(range(batch))\n",
    "for k in chosedfile:\n",
    "    chosedfile[k]=os.path.join('pic',random.choice(files))\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(3, 3))\n",
    "for k in chosedfile:\n",
    "    im=cv2.imread(chosedfile[k],cv2.CV_LOAD_IMAGE_GRAYSCALE)\n",
    "    gray=cv2.resize(255-im,(28,28))\n",
    "    (thresh, gray) = cv2.threshold(gray, 32, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)   \n",
    "    gray=centrelize(gray)\n",
    "    (thresh, gray) = cv2.threshold(gray, 32, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)   \n",
    "    gray=cv2.dilate(gray,kernel)\n",
    "    gray=gray/255\n",
    "    gray=(gray-0.5)*2\n",
    "    inputarray[k][0]=gray\n",
    "\n",
    "inputs=torch.FloatTensor(inputarray)\n",
    "imshow(torchvision.utils.make_grid(inputs))    \n",
    "\n",
    "#---------自行构造从目录读图的dataset以便使用dataloader\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                             ])\n",
    "#picset = PICSET(root='pic', transform=transform)\n",
    "#picloader = torch.utils.data.DataLoader(picset, batch_size=batch, shuffle=False, num_workers=1)\n",
    "#piciter = iter(picloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      " 6\n",
      " 1\n",
      " 5\n",
      "[torch.IntTensor of size 4x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mages = piciter.next()\n",
    "#=images.numpy()\n",
    "#rint a.shape\n",
    "#rint a[0][0]\n",
    "#mshow(torchvision.utils.make_grid(torch.FloatTensor(images.numpy()/2+0.5)))\n",
    "\n",
    "#images = torch.FloatTensor((1-(images.numpy()/2+0.5)-0.5)*2)\n",
    "       \n",
    "#imshow(torchvision.utils.make_grid(torch.FloatTensor((1-(images.numpy()/2+0.5)-0.5)*2)))     #from [-1,1] to [0,1] and to[-1,1]   \n",
    "\n",
    "outputs = model(Variable(inputs))\n",
    "_,predicted = torch.max(outputs.data, 1)\n",
    "#print chosedfile\n",
    "print predicted.int()\n",
    "#print classes[predicted.int()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded\n"
     ]
    }
   ],
   "source": [
    "#图像来自mnist test set\n",
    "#testset = mnist.MNIST(root='MNIST', train=False, download=True)\n",
    "#b=random.randint(0,mnist.MNIST.__len__(testset)-batch)\n",
    "#for i in range(0,batch):\n",
    "    #timg,tlabel=mnist.MNIST.__getitem__(testset,b+i)  #timg is PIL.Image\n",
    "    #inputarray[i][0]=np.asarray(list(timg.getdata())).reshape((28,28))\n",
    "    #inputarray[i][0]=inputarray[i][0]/255\n",
    "    #inputarray[i][0]=(inputarray[i][0]-0.5)*2\n",
    "#inputs=torch.FloatTensor(inputarray)\n",
    "#imshow(torchvision.utils.make_grid(inputs))        \n",
    "#-----------------------------------\n",
    "testset = mnist.MNIST(root='MNIST', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch, shuffle=False, num_workers=2)\n",
    "mnistiter = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB2CAYAAADY3GjsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5805282810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = mnistiter.next()\n",
    "imshow(torchvision.utils.make_grid(images))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 7\n",
      " 2\n",
      " 1\n",
      " 0\n",
      "[torch.IntTensor of size 4x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = model(Variable(images))\n",
    "_,predicted = torch.max(outputs.data, 1)\n",
    "print predicted.int()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
